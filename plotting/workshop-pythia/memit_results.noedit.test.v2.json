{"backpack-gpt2": {"company": {"intervention_score": 0.971764705882353, "general_score": 3.0086312770315313, "rest_of_prompt_score": 3.794720844703315, "hard_negative_score": 18.16425974320059}, "verbs": {"intervention_score": 0.4361111111111111, "general_score": 3.0086312770315313, "rest_of_prompt_score": 5.941955984327528, "hard_negative_score": 58.072379954655965}, "country": {"intervention_score": 0.9005145797598628, "general_score": 3.0086312770315313, "rest_of_prompt_score": 3.614709539909583, "hard_negative_score": 10.84307226859804}, "gender": {"intervention_score": 0.9111111111111111, "general_score": 3.0086312770315313, "rest_of_prompt_score": 5.2957504802684445, "hard_negative_score": 1.7321859990028625}, "temporal": {"intervention_score": 0.7695473251028807, "general_score": 3.0086312770315313, "rest_of_prompt_score": 3.0488655500775974, "hard_negative_score": 8.088368562154459}, "stereoset": {"intervention_score": 0.23836657169990502, "general_score": 3.0086312770315313, "rest_of_prompt_score": 5.825229625690338, "hard_negative_score": 51.97175542898441}}, "pythia-160m": {"country": {"intervention_score": 1.0, "general_score": 3.7435993541498407, "rest_of_prompt_score": 4.129254829806808, "hard_negative_score": 13.178879310344827}, "company": {"intervention_score": 1.0, "general_score": 3.7435993541498407, "rest_of_prompt_score": 4.423865176151762, "hard_negative_score": 23.739155251141554}, "temporal": {"intervention_score": 0.8868312757201646, "general_score": 3.7435993541498407, "rest_of_prompt_score": 3.530862341177355, "hard_negative_score": 11.133190321738432}, "stereoset": {"intervention_score": 0.20322886989553657, "general_score": 3.7435993541498407, "rest_of_prompt_score": 6.038885607403032, "hard_negative_score": 58.34852801519468}, "verbs": {"intervention_score": 0.5444444444444444, "general_score": 3.7435993541498407, "rest_of_prompt_score": 6.381712147887324, "hard_negative_score": 65.3404513888889}, "gender": {"intervention_score": 0.30277777777777776, "general_score": 3.7435993541498407, "rest_of_prompt_score": 5.660381087919038, "hard_negative_score": 2.745977393617021}}, "pythia-1.4b": {"gender": {"intervention_score": 0.8722222222222222, "general_score": 2.619850755525974, "rest_of_prompt_score": 5.000632511068944, "hard_negative_score": 1.8268700132978724}, "temporal": {"intervention_score": 0.6337448559670782, "general_score": 2.619850755525974, "rest_of_prompt_score": 2.325088339222615, "hard_negative_score": 5.783980891759721}, "company": {"intervention_score": 0.96, "general_score": 2.619850755525974, "rest_of_prompt_score": 3.05835027100271, "hard_negative_score": 13.92433424300799}, "stereoset": {"intervention_score": 0.30009496676163344, "general_score": 2.619850755525974, "rest_of_prompt_score": 5.45869019492026, "hard_negative_score": 44.40307454890788}, "verbs": {"intervention_score": 0.5277777777777778, "general_score": 2.619850755525974, "rest_of_prompt_score": 5.650528169014085, "hard_negative_score": 48.94479166666667}, "country": {"intervention_score": 0.8078902229845626, "general_score": 2.619850755525974, "rest_of_prompt_score": 2.609820607175713, "hard_negative_score": 5.812830431707974}}, "pythia-6.9b": {"company": {"intervention_score": 0.9129411764705883, "general_score": 2.4332698198435527, "rest_of_prompt_score": 2.854759485094851, "hard_negative_score": 11.139978417522832}, "gender": {"intervention_score": 0.8944444444444445, "general_score": 2.4332698198435527, "rest_of_prompt_score": 4.986737033523086, "hard_negative_score": 1.7151429521276595}, "country": {"intervention_score": 0.6380789022298456, "general_score": 2.4332698198435527, "rest_of_prompt_score": 2.4782658693652255, "hard_negative_score": 3.9563272410425645}, "verbs": {"intervention_score": 0.4861111111111111, "general_score": 2.4332698198435527, "rest_of_prompt_score": 5.586597711267606, "hard_negative_score": 47.41961805555555}, "stereoset": {"intervention_score": 0.30864197530864196, "general_score": 2.4332698198435527, "rest_of_prompt_score": 5.529028844260681, "hard_negative_score": 38.57656695156695}, "temporal": {"intervention_score": 0.5390946502057613, "general_score": 2.4332698198435527, "rest_of_prompt_score": 2.1294639500789416, "hard_negative_score": 4.592240282073425}}, "pythia-410m": {"verbs": {"intervention_score": 0.5472222222222223, "general_score": 2.8781628083944515, "rest_of_prompt_score": 5.684253961267606, "hard_negative_score": 51.84322916666667}, "stereoset": {"intervention_score": 0.28110161443494774, "general_score": 2.8781628083944515, "rest_of_prompt_score": 5.488961901949202, "hard_negative_score": 48.18435422602089}, "temporal": {"intervention_score": 0.7242798353909465, "general_score": 2.8781628083944515, "rest_of_prompt_score": 2.6220584918427186, "hard_negative_score": 7.4549984184206295}, "company": {"intervention_score": 0.9811764705882353, "general_score": 2.8781628083944515, "rest_of_prompt_score": 3.388253726287263, "hard_negative_score": 17.94056792237443}, "gender": {"intervention_score": 0.9222222222222223, "general_score": 2.8781628083944515, "rest_of_prompt_score": 5.0063646426312465, "hard_negative_score": 1.9769780585106382}, "country": {"intervention_score": 0.9176672384219554, "general_score": 2.8781628083944515, "rest_of_prompt_score": 2.932957681692732, "hard_negative_score": 8.399649784482758}}, "pythia-1b": {"stereoset": {"intervention_score": 0.2905982905982906, "general_score": 2.7072826208017906, "rest_of_prompt_score": 5.539587025004923, "hard_negative_score": 45.62226970560304}, "temporal": {"intervention_score": 0.6831275720164609, "general_score": 2.7072826208017906, "rest_of_prompt_score": 2.4219983459890235, "hard_negative_score": 6.381308430571176}, "country": {"intervention_score": 0.8507718696397941, "general_score": 2.7072826208017906, "rest_of_prompt_score": 2.8517709291628335, "hard_negative_score": 6.992273791082974}, "company": {"intervention_score": 0.971764705882353, "general_score": 2.7072826208017906, "rest_of_prompt_score": 3.1298272357723578, "hard_negative_score": 15.055079908675799}, "gender": {"intervention_score": 0.8944444444444445, "general_score": 2.7072826208017906, "rest_of_prompt_score": 5.030973276407337, "hard_negative_score": 1.9120262632978724}, "verbs": {"intervention_score": 0.5194444444444445, "general_score": 2.7072826208017906, "rest_of_prompt_score": 5.693056778169014, "hard_negative_score": 49.97847222222222}}, "pythia-70m": {"country": {"intervention_score": 0.9965694682675815, "general_score": 5.328501239480092, "rest_of_prompt_score": 5.668583256669733, "hard_negative_score": 19.705145474137932}, "gender": {"intervention_score": 0.08611111111111111, "general_score": 5.328501239480092, "rest_of_prompt_score": 7.299414927261227, "hard_negative_score": 4.0950797872340425}, "verbs": {"intervention_score": 0.6027777777777777, "general_score": 5.328501239480092, "rest_of_prompt_score": 7.513754401408451, "hard_negative_score": 92.11979166666667}, "temporal": {"intervention_score": 0.9567901234567902, "general_score": 5.328501239480092, "rest_of_prompt_score": 5.204571084880836, "hard_negative_score": 16.39739315552699}, "company": {"intervention_score": 1.0, "general_score": 5.328501239480092, "rest_of_prompt_score": 5.980013550135501, "hard_negative_score": 32.368721461187214}, "stereoset": {"intervention_score": 0.09971509971509972, "general_score": 5.328501239480092, "rest_of_prompt_score": 7.551572652096869, "hard_negative_score": 84.46498100664768}}, "pythia-2.8b": {"verbs": {"intervention_score": 0.5, "general_score": 2.5039758733216, "rest_of_prompt_score": 5.606734154929577, "hard_negative_score": 47.905902777777776}, "country": {"intervention_score": 0.6843910806174958, "general_score": 2.5039758733216, "rest_of_prompt_score": 2.4582566697332107, "hard_negative_score": 4.725055563038793}, "company": {"intervention_score": 0.9411764705882353, "general_score": 2.5039758733216, "rest_of_prompt_score": 2.938008130081301, "hard_negative_score": 11.72793704516267}, "stereoset": {"intervention_score": 0.3114909781576448, "general_score": 2.5039758733216, "rest_of_prompt_score": 5.554156822209096, "hard_negative_score": 41.55727682811016}, "gender": {"intervention_score": 0.8722222222222222, "general_score": 2.5039758733216, "rest_of_prompt_score": 4.9942678684376975, "hard_negative_score": 1.715442154255319}, "temporal": {"intervention_score": 0.5905349794238683, "general_score": 2.5039758733216, "rest_of_prompt_score": 2.19893241109691, "hard_negative_score": 5.083058648979756}}}