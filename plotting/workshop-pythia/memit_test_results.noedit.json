{"backpack-gpt2": {"company": {"intervention_score": 0.971764705882353, "general_score": 3.0086312770315313, "rest_of_prompt_score": 3.794720844703315, "hard_negative_score": 18.16425974320059}, "verbs": {"intervention_score": 0.4361111111111111, "general_score": 3.0086312770315313, "rest_of_prompt_score": 5.941955984327528, "hard_negative_score": 58.072379954655965}, "country": {"intervention_score": 0.9005145797598628, "general_score": 3.0086312770315313, "rest_of_prompt_score": 3.614709539909583, "hard_negative_score": 10.84307226859804}, "gender": {"intervention_score": 0.9111111111111111, "general_score": 3.0086312770315313, "rest_of_prompt_score": 5.2957504802684445, "hard_negative_score": 1.7321859990028625}, "temporal": {"intervention_score": 0.7695473251028807, "general_score": 3.0086312770315313, "rest_of_prompt_score": 3.0488655500775974, "hard_negative_score": 8.088368562154459}, "stereoset": {"intervention_score": 0.23836657169990502, "general_score": 3.0086312770315313, "rest_of_prompt_score": 5.825229625690338, "hard_negative_score": 51.97175542898441}}, "pythia-160m": {"country": {"intervention_score": 0.9965694682675815, "general_score": 3.2457497892985967, "rest_of_prompt_score": 3.8561380082791072, "hard_negative_score": 11.830303975220385}, "company": {"intervention_score": 1.0, "general_score": 3.2457497892985967, "rest_of_prompt_score": 3.8936804768838855, "hard_negative_score": 21.456521709215696}, "temporal": {"intervention_score": 0.8353909465020576, "general_score": 3.2457497892985967, "rest_of_prompt_score": 3.0433745711524276, "hard_negative_score": 9.62054341081482}, "stereoset": {"intervention_score": 0.22792022792022792, "general_score": 3.2457497892985967, "rest_of_prompt_score": 5.577815779129993, "hard_negative_score": 52.594166560956445}, "verbs": {"intervention_score": 0.575, "general_score": 3.2457497892985967, "rest_of_prompt_score": 5.679553849596373, "hard_negative_score": 57.759695784250894}, "gender": {"intervention_score": 0.9277777777777778, "general_score": 3.2457497892985967, "rest_of_prompt_score": 5.164269930799123, "hard_negative_score": 1.9713209276503705}}, "pythia-1.4b": {"gender": {"intervention_score": 0.8722222222222222, "general_score": 2.615755858563488, "rest_of_prompt_score": 5.0022146923492254, "hard_negative_score": 1.8034395622446182}, "temporal": {"intervention_score": 0.6358024691358025, "general_score": 2.615755858563488, "rest_of_prompt_score": 2.319773261667723, "hard_negative_score": 5.767216873461151}, "company": {"intervention_score": 0.96, "general_score": 2.615755858563488, "rest_of_prompt_score": 3.0471466782939465, "hard_negative_score": 13.892228829159857}, "stereoset": {"intervention_score": 0.3010446343779677, "general_score": 2.615755858563488, "rest_of_prompt_score": 5.4682544295913145, "hard_negative_score": 44.267452715468885}, "verbs": {"intervention_score": 0.49722222222222223, "general_score": 2.615755858563488, "rest_of_prompt_score": 5.648656251564832, "hard_negative_score": 48.92469362152947}, "country": {"intervention_score": 0.8096054888507719, "general_score": 2.615755858563488, "rest_of_prompt_score": 2.610255345798131, "hard_negative_score": 5.807545189231891}}, "pythia-6.9b": {"company": {"intervention_score": 0.8988235294117647, "general_score": 2.41024465142832, "rest_of_prompt_score": 2.8351753048780486, "hard_negative_score": 10.855067144245861}, "gender": {"intervention_score": 0.8888888888888888, "general_score": 2.41024465142832, "rest_of_prompt_score": 4.991421568627451, "hard_negative_score": 1.6733304936835107}, "country": {"intervention_score": 0.6380789022298456, "general_score": 2.41024465142832, "rest_of_prompt_score": 2.4480077046918125, "hard_negative_score": 3.8445141890953325}, "verbs": {"intervention_score": 0.4638888888888889, "general_score": 2.41024465142832, "rest_of_prompt_score": 5.561131437059859, "hard_negative_score": 47.11877170138889}, "stereoset": {"intervention_score": 0.3190883190883191, "general_score": 2.41024465142832, "rest_of_prompt_score": 5.557686983165977, "hard_negative_score": 38.045791785375116}, "temporal": {"intervention_score": 0.5473251028806584, "general_score": 2.41024465142832, "rest_of_prompt_score": 2.104588000902188, "hard_negative_score": 4.491205205647682}}, "pythia-410m": {"verbs": {"intervention_score": 0.5361111111111111, "general_score": 2.8604327787518695, "rest_of_prompt_score": 5.683046257831681, "hard_negative_score": 51.658746409416196}, "stereoset": {"intervention_score": 0.2839506172839506, "general_score": 2.8604327787518695, "rest_of_prompt_score": 5.51762351418931, "hard_negative_score": 47.58034623699424}, "temporal": {"intervention_score": 0.7345679012345679, "general_score": 2.8604327787518695, "rest_of_prompt_score": 2.606007948180588, "hard_negative_score": 7.387360285622831}, "company": {"intervention_score": 0.9788235294117648, "general_score": 2.8604327787518695, "rest_of_prompt_score": 3.3631897877225385, "hard_negative_score": 17.776479594783694}, "gender": {"intervention_score": 0.9277777777777778, "general_score": 2.8604327787518695, "rest_of_prompt_score": 5.003785865055014, "hard_negative_score": 1.941498064994812}, "country": {"intervention_score": 0.9125214408233276, "general_score": 2.8604327787518695, "rest_of_prompt_score": 2.8902273204420945, "hard_negative_score": 8.379993548167162}}, "pythia-1b": {"stereoset": {"intervention_score": 0.2943969610636277, "general_score": 2.7071153419588256, "rest_of_prompt_score": 5.551526958910962, "hard_negative_score": 45.609036307049614}, "temporal": {"intervention_score": 0.6872427983539094, "general_score": 2.7071153419588256, "rest_of_prompt_score": 2.422623900128221, "hard_negative_score": 6.38576335465095}, "country": {"intervention_score": 0.8524871355060034, "general_score": 2.7071153419588256, "rest_of_prompt_score": 2.856244042474629, "hard_negative_score": 6.9933361921395205}, "company": {"intervention_score": 0.971764705882353, "general_score": 2.7071153419588256, "rest_of_prompt_score": 3.1256525044841816, "hard_negative_score": 15.059103584725019}, "gender": {"intervention_score": 0.8916666666666667, "general_score": 2.7071153419588256, "rest_of_prompt_score": 5.027684030164879, "hard_negative_score": 1.9093200343720456}, "verbs": {"intervention_score": 0.5194444444444445, "general_score": 2.7071153419588256, "rest_of_prompt_score": 5.694272451837298, "hard_negative_score": 50.01939575937059}}, "pythia-70m": {"country": {"intervention_score": 0.9982847341337907, "general_score": 3.6456239253861753, "rest_of_prompt_score": 4.0103610897678355, "hard_negative_score": 14.590774110678968}, "gender": {"intervention_score": 0.95, "general_score": 3.6456239253861753, "rest_of_prompt_score": 5.287061840276941, "hard_negative_score": 2.202018841784051}, "verbs": {"intervention_score": 0.7416666666666667, "general_score": 3.6456239253861753, "rest_of_prompt_score": 5.983031481084689, "hard_negative_score": 65.16642349825965}, "temporal": {"intervention_score": 0.9053497942386831, "general_score": 3.6456239253861753, "rest_of_prompt_score": 3.5154909438346977, "hard_negative_score": 11.500596495704357}, "company": {"intervention_score": 1.0, "general_score": 3.6456239253861753, "rest_of_prompt_score": 4.3519510318270225, "hard_negative_score": 24.611325764764935}, "stereoset": {"intervention_score": 0.18328584995251662, "general_score": 3.6456239253861753, "rest_of_prompt_score": 5.769464814069116, "hard_negative_score": 66.71542346443546}}, "pythia-2.8b": {"verbs": {"intervention_score": 0.49722222222222223, "general_score": 2.4979796341116125, "rest_of_prompt_score": 5.601926991637324, "hard_negative_score": 47.875325520833336}, "country": {"intervention_score": 0.6912521440823327, "general_score": 2.4979796341116125, "rest_of_prompt_score": 2.4722717341306346, "hard_negative_score": 4.686115264892578}, "company": {"intervention_score": 0.9364705882352942, "general_score": 2.4979796341116125, "rest_of_prompt_score": 2.930058011517615, "hard_negative_score": 11.698520050745577}, "stereoset": {"intervention_score": 0.30959164292497626, "general_score": 2.4979796341116125, "rest_of_prompt_score": 5.567065367198268, "hard_negative_score": 41.254258665716996}, "gender": {"intervention_score": 0.8805555555555555, "general_score": 2.4979796341116125, "rest_of_prompt_score": 4.976023877292852, "hard_negative_score": 1.7274060837765957}, "temporal": {"intervention_score": 0.5946502057613169, "general_score": 2.4979796341116125, "rest_of_prompt_score": 2.1929366213066688, "hard_negative_score": 5.060827201313715}}}